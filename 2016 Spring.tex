\documentclass{article} 	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage[UTF8, heading = false, scheme = plain]{ctex}
\usepackage[colorlinks = false]{hyperref}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{2016 Spring Notes}
\author{Yue Yu}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{各种相关背景知识}
	\subsection{概率论}
		\subsubsection{条件概率期望:}
			\begin{eqnarray}
			E(X) = E(E(X|Y))
			\end{eqnarray}
		\subsubsection{Correlation coefficient(相关系数)}
			\begin{eqnarray}
				\rho_{\scriptscriptstyle X,\scriptscriptstyle Y} &=& \frac{Cov(X,Y)}{\sigma_{\scriptscriptstyle X}\sigma_{\scriptscriptstyle Y}}\\
				&=& \frac{E(X-E(X))(Y-E(Y))}{\sigma_{\scriptscriptstyle X}\sigma_{\scriptscriptstyle Y}}\\
				&=& \frac{E(XY)-E(X)E(Y)}{\sigma_{\scriptscriptstyle X}\sigma_{\scriptscriptstyle Y}}
			\end{eqnarray}
		\subsubsection{协方差矩阵}
                        \begin{eqnarray}
                        	       \mathrm{X} &=& [X_1,\ldots,X_n]^T\\
                                \Sigma &=& \mathrm{E}\Big[(\mathrm{X}-\mathrm{E}(\mathrm{X}))(\mathrm{X}-\mathrm{E}(\mathrm{X}))^T\Big]\\
                                \Sigma_{i,j} &=& Cov(X_i, X_j)\\
                                &=& \mathrm{E}\Big[(X_i - \mu_i)(X_j - \mu_j)\Big]
                        \end{eqnarray}

                \subsubsection{Multivariate normal distribution}
                        \begin{itemize}
                                \item
                                多维高斯联合分布:
                                $$f_X(x_1,\ldots,x_n) = \frac{1}{\sqrt{(2\pi)^k|\Sigma|}}\mathrm{exp}(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$
                                
                                \item
                                当为二维高斯分布时（$\rho$为相关系数）:\\
                                $$f(x,y) = \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\mathrm{exp}\Big( -\frac{1}{2(1-\rho)^2} 
                                \Big[\frac{(x-\mu_x)}{\sigma_x^2}+\frac{(y-\mu_y)}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}\Big]\Big)$$
                                
                                条件概率服从:
                                $$P(X_1\big|X_2 =x_2)\sim N(\mu_1 +\frac{\sigma_1}{\sigma_2}\rho(x_2-\mu_2),(1-\rho^2)\sigma_1^2)$$
                        \end{itemize}
	        
	        \subsubsection{Laplace Distribution:}
	        		\begin{itemize}
			\item
			概率密度函数\\
			$$f(x|\mu,b)= \displaystyle\frac{1}{2b} \mathrm{exp}\big(-\frac{|x-\mu|}{b}\big)$$
			\item
			期望\\
			$$\mu$$
			\item
			方差\\
			$$2b^2$$
			\end{itemize}
	
	\subsection{矩阵求导法则：}
		\begin{eqnarray}
		\mathbf{\frac{\partial u^TAv}{\partial x}} &=& \mathbf{\frac{\partial u}{\partial x}Av + \frac{\partial v}{\partial x}A^Tu}\\
		\mathbf{\frac{\partial(u(x)+v(x))}{\partial x}} &=& \mathbf{\frac{\partial u(x)}{\partial x}+ \frac{\partial v(x)}{\partial x} }\\
		\mathbf{\frac{\partial Ax}{\partial x}} &=& \mathbf{A^T}\\
		\mathbf{\frac{\partial a}{\partial x}} &=& \mathbf{0} \\
		\mathbf{\frac{\partial x^T Ab}{\partial x}} &=& \mathbf{Ab}\\
		\mathbf{\frac{\partial x^TAx}{\partial x}} &=& \mathbf{(A+A^T)x}\\
		&=& \mathbf{2Ax} \textrm{\qquad 如果A为对称阵}\\
		\mathbf{\frac{\partial |X|}{\partial X}} &=& \mathbf{|X|(X^{-1})^T} \\
		\mathbf{\frac{\partial \ln |X|}{\partial X}} &=& \mathbf{(X^{-1})^T}
		%\mathbf{\frac{\partial}{\partial x}} &=& \mathbf{\frac{\partial}{\partial x}} 
		\end{eqnarray}
	
\newpage




\section{Information Theory}
	\subsection{Differential Entropy}
		\subsubsection{常见 Differential Entropy}
			\begin{itemize}
			       \item
            		       \emph{Uniform distribution(from 0 to a)}
                                \[
                                h(X) = \log(a)
                                \]
                                
                                \item
                                \emph{Normal Distribution}
                                \[
                                 X\sim N(0,\sigma^2)
                                 \]
                                 \[
                                 h(X) = \frac{1}{2} \log2\pi e \sigma^2 
                                \]
                                
                                \item
                                \emph{Multivariate Normal Distribution}
                                $$N_n\sim(\mu, K)$$
                                $\mu$ is mean and $K$ is covariance matrix
                                \[
                                h(X_1,\ldots,X_n) = \frac{1}{2}\log(2\pi e )^n |K|
                                \]
			\end{itemize}
			
		\subsubsection{Properties}
                        \begin{itemize}
                                \item $h(X+c) = h(X)$\\
                                \item$h(aX) = h(X) + \log|a|$\\
                                \item$h(AX) = h(X) + \log\big|\mathrm{det}(A)\big|$
                        \end{itemize}

\newpage
\section{Machine Learning}
	\subsection{MAXIMUM LIKELIHOOD ESTIMATION (MLE)}
		\subsubsection{GAUSSIAN MLE}
			$$\hat{\mu}_{\scriptscriptstyle ML} = \frac{1}{n}\sum_{i=1}^nx_i$$
			$$\hat{\Sigma}_{\scriptscriptstyle ML} = \frac{1}{n}\sum_{i=1}^n(x_i - \hat{\mu}_{\scriptscriptstyle ML})(x_i - \hat{\mu}_{\scriptscriptstyle ML})^T$$
	
	\subsection{Linear Regression}
		\subsubsection{Least Squares}
			Usually, for linear regression (and classification) we include an intercept term $w_0$ that doesn’t interact with any element in the vector $x$. It will be 
			convenient to attach a 1 to the first dimension of each vector $x_i$.
			\[
			x_i = \left [
				\begin{array}{c}
				1\\
				x_{i1}\\
				\vdots\\
				x_{id}\\
				\end{array}
				\right ]
			, \qquad
			\mathbf{X} = \left [
					    \begin{array}{cccc}
					    1 &x_{11}& \ldots & x_{1d}\\
					    1 &x_{21}& \ldots & x_{2d}\\
					    \vdots &\vdots&  & \vdots\\
					    1 &x_{n1}& \ldots & x_{nd}\\
					    \end{array}
					    \right]
			\]
			这种情况下，得到的解为:
			\[
			w_{\scriptscriptstyle \mathrm{ML}} = (X^TX)^{-1}X^Ty
			\]
			预测新的点为:
			$$y_{\scriptscriptstyle\mathrm{new}} = x_{\scriptscriptstyle\mathrm{new}}^Tw_{\scriptscriptstyle \mathrm{ML}} $$
			
	\subsection{Classification}
		\subsubsection{Bayes Classifier}
			The Bayes classifier has the smallest prediction error of all classifiers.\\
			假设$(X,Y)$独立同分布，那么optimal classifier is:
			\begin{eqnarray}
			f(x) = \argmax_{y\in \mathcal{Y}} P(Y=y|X=x)
			\end{eqnarray} 			Using Bayes rule and ignoring $P(X = x)$ we equivalently have: 
			\begin{eqnarray}
			f(x) = \argmax_{y\in \mathcal{Y}} P(Y=y)\times P(X=x|Y=y)
			\end{eqnarray}
			其中$P(Y=y)$叫做 class prior，$P(X=x|Y=y)$叫做 data likelihood given class.
		
	
	
			
		
	


\end{document}  